{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie Wars\n",
    "## ~ Episode III â€“ Revenge of the Outliers  ~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we should set the notebook so that it outputs all results of each cell and not only the last one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And import all the python libraries needed for this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we state where our data sources are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder_path = 'data\\\\'\n",
    "movies_file_path = data_folder_path + 'movies_with_genre_and_year.csv'\n",
    "users_file_path = data_folder_path + 'users_with_age_interval_and_occupation.csv'\n",
    "ratings_file_path = data_folder_path + 'ratings.csv'\n",
    "ratings_by_user_file_path  = data_folder_path + 'ratings_by_user_data.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_movies = pd.read_csv(movies_file_path, sep = ';', index_col = 'ID')\n",
    "all_users = pd.read_csv(users_file_path, sep = ';', index_col = 'Id')\n",
    "all_ratings = pd.read_csv(ratings_file_path, sep = ',')\n",
    "ratings_by_user = pd.read_csv(ratings_by_user_file_path, sep = ';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are ready to start with the feature engineering process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing the class mark for the users' age"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve the age feature we can set the **age** class mark as de midpoint of the interval rather than a extreme of the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "centered_user_ages = {1: (0 + 17)/2, 18: (18 + 24)/2, 25: (25 + 34)/2, 35: (35 + 44)/2, 45: (45 + 49)/2, 50: (50 + 55)/2, 56: (56 + 70)/2}\n",
    "\n",
    "all_users['Age_centered'] = all_users.Age.map(lambda x: centered_user_ages[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping the users' occupation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve the occupation feature we can regroup its classes in **fewer categories** and make them **more balanced**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "user_occupations_categories = {1: 'artist', 2: 'craftsmen', 3: 'engineer', 4: 'academic', 5: 'student', 6: 'customer-fancing',\n",
    "                       7: 'other', 8:'unemployed', 9: 'high-wage'}\n",
    "\n",
    "occupations_map = {20: 1, 2: 1, 18: 2, 8: 2, 9: 2, 12: 3, 17: 3, 15:4 ,1: 4, 10: 5, 4: 5, 16: 6, 14: 6, 5: 6, 0: 7, 3: 7, \n",
    "                  19: 8, 13: 8, 6: 9, 7: 9, 11: 9}\n",
    "\n",
    "all_users['Occupation_categorie'] = all_users['Occupation'].map(lambda x: occupations_map[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how it looks now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_users['Occupation_categorie_decoded'] = all_users['Occupation_categorie'].map(lambda x: user_occupations_categories[x])\n",
    "\n",
    "occupation_categorie_distribution_dict = Counter(all_users.Occupation_categorie_decoded)\n",
    "occupation_categorie_distribution_dict_sorted = {k: v for k, v in sorted(occupation_categorie_distribution_dict.items(), key=lambda item: item[1], reverse = True)}\n",
    "\n",
    "fig, axs = plt.subplots(1, 1, figsize=(12,12))\n",
    "_ = axs.pie(occupation_categorie_distribution_dict_sorted.values(), labels = occupation_categorie_distribution_dict_sorted.keys(), autopct='%1.1f%%')\n",
    "_ = axs.set_title('Distribution of users by occupation categorie')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving the profiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Users\n",
    "\n",
    "We can add information to the user profile by adding their **average rating per movie genre** and an interval for the **years** of the movies that the have rated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_users['id_movies_rated'] = all_users.index.map(lambda x: list(all_ratings[all_ratings['user'] == x]['movie']))\n",
    "all_users['count_movies_rated'] = all_users['id_movies_rated'].map(lambda x: len(x))\n",
    "\n",
    "\n",
    "Movie_genres = ['Action','Adventure','Animation',\"Children's\",'Comedy','Crime','Documentary','Drama','Fantasy','Film-Noir','Horror',\n",
    " 'Musical','Mystery','Romance','Sci-Fi','Thriller','War','Western']\n",
    "\n",
    "for genre in Movie_genres:\n",
    "    Movies_genre = all_movies[all_movies[genre] > 0].index\n",
    "    all_users[genre] = all_users.id_movies_rated.apply(lambda x: round(all_ratings[all_ratings['movie'].isin(set(x) & set(Movies_genre))].rating.mean(), 3))\n",
    "    all_users[genre + '_count'] = all_users.id_movies_rated.map(lambda x: len(all_movies[(all_movies.index.isin(x)) & (all_movies[genre] == 1)]))\n",
    "\n",
    "all_users['Epoch_of_rated_movies'] = all_users['id_movies_rated'].map(lambda x: round(all_movies[all_movies.index.isin(list(x))]['Year'].mean(), 3))\n",
    "all_users['Epoch_of_rated_movies_std'] = all_users['id_movies_rated'].map(lambda x: round(all_movies[all_movies.index.isin(list(x))]['Year'].std(), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Affinity coefficient\n",
    "\n",
    "We use the **genre** of each film and the **average rating** of each user for each genre to create an affinity coefficient between each film and each user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for genre in Movie_genres:\n",
    "    all_ratings[genre + '_user_affinity'] = all_ratings.apply(lambda row: round(all_users.at[int(row['user']), genre] * all_movies.at[int(row['movie']), genre], 3), axis = 1)\n",
    "    all_ratings[genre] = all_ratings.apply(lambda row: all_movies.at[int(row['movie']), genre], axis = 1)\n",
    "\n",
    "all_ratings['genre_affinity'] = all_ratings[[genre + '_user_affinity' for genre in Movie_genres]].mean(axis = 1)\n",
    "all_ratings['user_age'] = all_ratings['user'].apply(lambda x: all_users.at[x, 'Age'])\n",
    "all_ratings['user_gender'] = all_ratings['user'].apply(lambda x: all_users.at[x, 'Gender'])\n",
    "all_ratings['user_occupation_categorie'] = all_ratings['user'].apply(lambda x: all_users.at[x, 'Occupation_categorie'])\n",
    "all_ratings['user_movies_epoch'] = all_ratings['user'].apply(lambda x: all_users.at[x, 'Epoch_of_rated_movies'])\n",
    "all_ratings['movie_year'] = all_ratings['movie'].apply(lambda x: all_movies.at[x, 'Year'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thankfully in a recommendation system based on ratings, neither missing values nor measurement noise exists. \n",
    "\n",
    "But there are still some things that we can do to improve the quality of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers\n",
    "\n",
    "Outlier are a data points that differ significantly from other observations. In the case of our users, we can find two groups of them:\n",
    "- The *\"haters\"* that rate everything negatively\n",
    "- The *\"lovers\"* that rate everything as perfect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of haters: 30 (0.5) %\n",
      "Number of lovers: 32 (0.53) %\n"
     ]
    }
   ],
   "source": [
    "haters = list(ratings_by_user[(ratings_by_user['mean'] < 2.5) & (ratings_by_user['std'] < 2)].index)\n",
    "lovers = list(ratings_by_user[(ratings_by_user['mean'] > 4) & (ratings_by_user['std'] < 0.6)].index)\n",
    "print(f\"Number of haters: {len(haters)} ({round(100*len(haters)/len(all_users), 2)}%)\")\n",
    "print(f\"Number of lovers: {len(lovers)} ({round(100*len(lovers)/len(all_users), 2)}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For models based on **user profiling**, we need to **remove** this anomalous users to improve the gereralization of the profiles. But for models based on **closest neighbors**, those anomalous users can be actually very helpful so is better to **keep** them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Lack of ratings\n",
    "\n",
    "For models based on user profiling, it is also important to **remove** users with very few ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "People with less than 5 ratings: 2792 (46.23) %\n"
     ]
    }
   ],
   "source": [
    "users_with_less_than_5_ratings = list(all_users[all_users['count_movies_rated'] < 5].index)\n",
    "print(f\"People with less than 5 ratings: {len(users_with_less_than_5_ratings)} ({round(100*len(users_with_less_than_5_ratings)/len(all_users), 2)}% )\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spliting the data\n",
    "\n",
    "We can consider two ways of spliting our data set into 80% training and 20% testing sets, the first option is just to do a **random sampling split**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "classic_train_data, classic_test_data = train_test_split(all_ratings, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second one is to first add **at least one rating** for each film and one rating for each user to the training set and **fill the remaining capacity up to 80% randomly**, this warranties no movie nor user in our test set will not have a rating.\n",
    " \n",
    "The second approach must be taken when the model selected cannot handle unseen movies/users during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Movie_Ids = list(all_ratings['movie'].unique())\n",
    "Users = list(all_ratings['user'].unique())\n",
    "\n",
    "# Take at least one rating per user and film\n",
    "minimum_train_data_ids = []\n",
    "remaining_data_ids = []\n",
    "\n",
    "users_in_minimum_train_data = set()\n",
    "movies_in_minimum_train_data = set()\n",
    "\n",
    "shuffled_ratings = shuffle(all_ratings)\n",
    "\n",
    "for index, row in shuffled_ratings.iterrows():\n",
    "    if row.user not in users_in_minimum_train_data or row.movie not in movies_in_minimum_train_data :\n",
    "        minimum_train_data_ids.append(row.id)\n",
    "\n",
    "        users_in_minimum_train_data |= {row.user}\n",
    "        movies_in_minimum_train_data |= {row.movie}\n",
    "    else:\n",
    "        remaining_data_ids.append(row.id)\n",
    "        \n",
    "minimum_train_data = all_ratings[all_ratings.index.isin(minimum_train_data_ids)]\n",
    "remaining_data = all_ratings[~all_ratings.index.isin(minimum_train_data_ids)]\n",
    "\n",
    "# Random sample the remaining up to 80%\n",
    "L = len(all_ratings)\n",
    "l = len(minimum_train_data)\n",
    "remaining_train_data, special_test_data = train_test_split(remaining_data, test_size = 0.2*(L/(L-l)))\n",
    "\n",
    "special_train_data = minimum_train_data.append(remaining_train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to remove the outliers and users with few ratings **only from the training set**, and only for ceirtaing models. If we were to remove them from the test set, our metrics wouldn't be valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "classic_train_data_clean = classic_train_data[(~classic_train_data['user'].isin(haters)) & (~classic_train_data['user'].isin(users_with_less_than_5_ratings))]\n",
    "special_train_data_clean = special_train_data[(~special_train_data['user'].isin(haters)) & (~special_train_data['user'].isin(users_with_less_than_5_ratings))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we save the resulting training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_for_modelling = ['rating', 'user_age', 'user_gender', 'user_occupation_categorie', 'user_movies_epoch', 'movie_year', 'genre_affinity']\n",
    "features_for_modelling += Movie_genres\n",
    "\n",
    "classic_train_data[features_for_modelling].to_csv(data_folder_path + 'ratings_training_data_basic_split.csv', sep = ';')\n",
    "classic_train_data_clean[features_for_modelling].to_csv(data_folder_path + 'ratings_training_data_basic_split_clean.csv', sep = ';')\n",
    "classic_test_data[features_for_modelling].to_csv(data_folder_path + 'ratings_test_data_basic_split.csv', sep = ';')\n",
    "\n",
    "special_train_data[features_for_modelling].to_csv(data_folder_path +'ratings_training_data_special_split.csv', sep = ';')\n",
    "special_train_data_clean[features_for_modelling].to_csv(data_folder_path +'ratings_training_data_special_split_clean.csv', sep = ';')\n",
    "special_test_data[features_for_modelling].to_csv(data_folder_path +'ratings_test_data_special_split.csv', sep = ';')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
